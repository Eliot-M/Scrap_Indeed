{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des offres depuis le site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import packages --- #\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "from selenium import webdriver #used to create session\n",
    "from selenium.webdriver.chrome.options import Options # used for inconito mode\n",
    "from selenium.webdriver.common.keys import Keys # used to simulate keyboard keys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data scraper function --- #\n",
    "def getJobData():\n",
    "    '''\n",
    "    Function used to get information from xpath. Collected data: job title, url of detailed offer, city, date, description preview\n",
    "    \n",
    "    input:\n",
    "        None\n",
    "        \n",
    "    output: \n",
    "        new_df (df): pandas dataframe containing collected data (empty if any issue occurs during the scrap)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Get job titles and links\n",
    "    titles_element = browser.find_elements_by_xpath(\"//a[@class='jobtitle turnstileLink ']\")\n",
    "    titles = [x.text for x in titles_element]\n",
    "    links = [x.get_attribute(\"href\") for x in titles_element]\n",
    "\n",
    "    # Get company name\n",
    "    company_element = browser.find_elements_by_xpath(\"//span[@class='company']\")\n",
    "    company = [x.text for x in company_element]\n",
    "    \n",
    "    # Get city location\n",
    "    city_element = browser.find_elements_by_xpath(\"//div[@class='location'] | //span[@class='location']\")\n",
    "    city = [x.text for x in city_element]\n",
    "\n",
    "    # Get date of post\n",
    "    date_element = browser.find_elements_by_xpath(\"//div[@class='result-link-bar']\")\n",
    "    date = [x.text for x in date_element]\n",
    "    date = [re.sub(r' - sauvegarder.*$', r'', w) for w in date]\n",
    "    \n",
    "    # Get description\n",
    "    desc_element = browser.find_elements_by_xpath(\"//div[@class='summary']\")\n",
    "    desc = [x.text for x in desc_element]\n",
    "    \n",
    "        # If all field are complete (at least same length), store it\n",
    "    if (len(titles) == len(company)) and (len(titles) == len(city)) and (len(titles) == len(date)) and (len(titles) == len(desc)):\n",
    "        new_d = {'Job':titles, 'Company':company, 'Link':links, 'City':city, 'Posted':date, 'Description':desc}\n",
    "        new_df = pd.DataFrame(new_d)\n",
    "        print(str(browser.current_url) + \" : Done\")\n",
    "    else:\n",
    "        # Else return empty df, which have no impact and allow a unique format for return\n",
    "        new_df = pd.DataFrame({'Job':[], 'Company':[], 'Link':[], 'City':[], 'Posted':[], 'Description':[]})\n",
    "        print('Error in this url : ')\n",
    "        print(browser.current_url)\n",
    "        print('For information - posted : ' + str(len(date)) + ', titles : ' + str(len(titles)) + ', cities : ' + str(len(city)) + ', companies : ' + str(len(company)) + ', descriptions : ' + str(len(desc)) + '.')\n",
    "    \n",
    "    return new_df\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Crawler function --- #\n",
    "\n",
    "def ScrapPages(job, city, pages_to_scrap = 1, driver_path =\"/Users/eliotmoll/Documents/Data_Aticles_Pro/chromedriver\"):\n",
    "    '''\n",
    "    Function to scrap pages for specific research (Job and city) using selenium.\n",
    "    \n",
    "    input:\n",
    "        job (str): Job name.\n",
    "        city (str): City for the job.\n",
    "        pages_to_scrap (int): number of pages to scrap (at least 1). 1 will return the first page of result for the search.\n",
    "        driver_path (str): path to executable driver (chrome in this case).\n",
    "        \n",
    "    output:\n",
    "        df (df): pandas dataframe containing collected data and added infos like the date of the scraping (empty if any issue occurs during the scrap)\n",
    "    '''\n",
    "    \n",
    "    # Set inconito session and define path to webdriver exe.\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path = driver_path, chrome_options=chrome_options)\n",
    "\n",
    "    # Define entry page\n",
    "    browser.get(\"https://www.indeed.fr/\")\n",
    "    # Leave a little breaks for loading (1 to 2s)\n",
    "    time.sleep(round(random.random() + 1))\n",
    "\n",
    "    # Use input field to do the research\n",
    "    inputElement = browser.find_element_by_id(\"text-input-what\")\n",
    "    inputElement.send_keys(job)\n",
    "\n",
    "    inputElement2 = browser.find_element_by_id(\"text-input-where\")\n",
    "    \n",
    "    # Remove all potential pre-write elements. 15 characters should be sufficient. Other selenium method seems to not work on mac+chrome\n",
    "    for x in range(15):\n",
    "        inputElement2.send_keys(Keys.BACK_SPACE)\n",
    "    inputElement2.send_keys(city)\n",
    "\n",
    "    time.sleep(round(random.random() + 1))\n",
    "\n",
    "    # click on the search button\n",
    "    browser.find_element_by_css_selector('.icl-Button.icl-Button--primary.icl-Button--md.icl-WhatWhere-button').click()\n",
    "\n",
    "    # Let the fun begin\n",
    "    df = pd.DataFrame({'Job':[], 'Company':[], 'Link':[], 'City':[], 'Posted':[], 'Description':[]})\n",
    "\n",
    "    # Reach all pages\n",
    "    for i in range(pages_to_scrap):\n",
    "        time.sleep(round(random.random() * 2 +1, 1))\n",
    "        # Get data, using the previously define function\n",
    "        df = df.append(getJobData())\n",
    "        # Find other pages links\n",
    "        pages_n = browser.find_elements_by_xpath(\"//div[@class='pagination']/a\")\n",
    "        pages = [x.get_attribute(\"href\") for x in pages_n]\n",
    "        # If it's not the last page to scrap, load the next one.\n",
    "        if i < pages_to_scrap:\n",
    "            browser.get(pages[-1]) # Last element correspond to \"Suivant/Next\".\n",
    "\n",
    "\n",
    "    browser.close()\n",
    "\n",
    "    # Add information for reading\n",
    "    df['isRead'] = 'no' # did you read this offer ? \n",
    "    df['New'] = 'yes' # is that a new offer (obtain from the last scrap) ?\n",
    "    df['More'] = 'no' # do you want more information (get detailled description) ?\n",
    "    df['MoreDone'] = 'no' # is the detailled scrap already done ?\n",
    "    df['FullDescription'] = '-'\n",
    "    # Add the date of the scrap session. Could also be used to determined the publish date of the offer\n",
    "    today = date.today()\n",
    "    scr_date = today.strftime(\"%d/%m/%Y\")\n",
    "    df['Date'] = scr_date\n",
    "\n",
    "    print(\"Overall Done\")\n",
    "    print(\"------------\")\n",
    "    print(df.shape)\n",
    "    \n",
    "    return(df)\n",
    "\n",
    "df = ScrapPages(job='Data Scientist', city='Paris', pages_to_scrap=2)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleaning function to improve indeed results --- #\n",
    "def cleaningOffers(input_df):\n",
    "    '''\n",
    "    Cleaning function to remove undesired offers.\n",
    "    \n",
    "    input:\n",
    "        input_df (df): dataframe with offers. Need to have 'Job', 'Company', 'Description', 'City'\n",
    "        \n",
    "    output:\n",
    "        input_df (df): dataframe with offers. Same format as input, just filtered.\n",
    "    '''\n",
    "    \n",
    "    print(input_df.shape)\n",
    "    # Convert fields to lowercase for search function\n",
    "    input_df['Job'] = input_df['Job'].str.lower()\n",
    "    input_df['Company'] = input_df['Company'].str.lower()\n",
    "    input_df['Description'] = input_df['Description'].str.lower()\n",
    "    input_df['City'] = input_df['City'].str.lower()\n",
    "    \n",
    "    \n",
    "    # Keep only data science related jobs\n",
    "    input_df = input_df[input_df['Job'].str.contains(\"scientist|science|research\")]\n",
    "    \n",
    "    # Remove non full-time jobs\n",
    "    input_df = input_df[~input_df['Job'].str.contains(\"alternan|stage|intern\")]\n",
    "    \n",
    "    # Remove undesired seniority or function (consulting)\n",
    "    input_df = input_df[~input_df['Job'].str.contains(\"lead|chief|chef|manager|senior|consult\")]\n",
    "    \n",
    "    input_df = input_df[~input_df['Company'].str.contains(\"consult|conseil\")]\n",
    "    input_df = input_df[~input_df['Description'].str.contains(\"consult|conseil\")]\n",
    "        # Remove specific companies (like consulting firms).\n",
    "    input_df = input_df[~input_df['Company'].str.contains(\"capgemini|novencia|kpmg|ey|mazars|accenture|sopra|avisia|ingeniance\")]\n",
    "    \n",
    "    # Remove specific cities (far from home with public transportation)\n",
    "    input_df = input_df[~input_df['City'].str.contains(\"villetaneuse\")]\n",
    "    \n",
    "    # Remove commas to be sure to not have any issue with csv output    \n",
    "    input_df.Job = input_df.Job.apply(lambda x: x.replace(',',' '))\n",
    "    input_df.Description = input_df.Description.apply(lambda x: x.replace(',',' '))\n",
    "    \n",
    "    print(input_df.shape)\n",
    "    \n",
    "    return(input_df)\n",
    "\n",
    "df = cleaningOffers(df)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Add new offer to old ones --- #\n",
    "# Load stored data\n",
    "dfs = pd.read_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/Jobs/Scraps/job_found.csv')\n",
    "\n",
    "# Change details, all loaded data are not anymore the most recent ones.\n",
    "dfs['New'] = 'no'\n",
    "\n",
    "# Add new offers\n",
    "dfs = dfs.append(df)\n",
    "\n",
    "# Remove overlapping jobs (based on Job, Company, City and Description). Keep the first one (which is the \"old\" one).\n",
    "dfs = dfs.drop_duplicates(subset=['Job', 'Company', 'City', 'Description'], keep='first')\n",
    "\n",
    "# Store back data\n",
    "dfs.to_csv(\"/Users/eliotmoll/Documents/Data_Aticles_Pro/Jobs/Scraps/job_found.csv\", index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check new data\n",
    "dfs = pd.read_csv('/Users/eliotmoll/Documents/Data_Aticles_Pro/Jobs/Scraps/job_found.csv')\n",
    "\n",
    "dfs[dfs.New == 'yes']\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define interesting offers\n",
    "\n",
    "dfs.set_value(1, 'More', 'yes') # row index, column name, new value\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New selenium scraper for intersting offers --- #\n",
    "def ScrapDetailPages(df, driver_path =\"/Users/eliotmoll/Documents/Data_Aticles_Pro/chromedriver\"):  \n",
    "    '''\n",
    "    Function to add full description information. \n",
    "    \n",
    "    input:\n",
    "        df (df): dataframe with offers. Need to have 'More', 'Link' and 'MoreDone'.\n",
    "        driver_path (str): path to executable driver (chrome in this case).\n",
    "    \n",
    "    output:\n",
    "        df (df): dataframe with offers and detailled description.\n",
    "    '''\n",
    "    \n",
    "    # Set inconito session and define path to webdriver exe.\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--incognito\")\n",
    "\n",
    "    browser = webdriver.Chrome(executable_path=\"/Users/eliotmoll/Documents/Data_Aticles_Pro/chromedriver\", chrome_options=chrome_options)\n",
    "    \n",
    "    # Get informations for specific offers\n",
    "    for x in df[(df.More == 'yes') & (df.MoreDone == 'no')].Link:\n",
    "\n",
    "        # Get url\n",
    "        browser.get(str(x))\n",
    "        \n",
    "        time.sleep(round(random.random() + 1))\n",
    "\n",
    "        # Get full description \n",
    "        details_element = browser.find_elements_by_xpath(\"//div[@class='jobsearch-jobDescriptionText']\")\n",
    "        details = [x.text for x in details_element]\n",
    "            # Remove commas to be sure to not have any issue with csv output  \n",
    "        details_clean = details[0].replace(\",\", \" \")\n",
    "        \n",
    "        # Find indices to replace empty value with scraping results\n",
    "        #indices = df.index[df['Links'] == x].tolist()\n",
    "        #for i in indices:\n",
    "            #df.set_value(i, 'FullDescription', details_clean)\n",
    "        \n",
    "        # Find indices to replace empty value with scraping results\n",
    "        df[df.Link == x].FullDescription = details_clean\n",
    "        df[df.Link == x].MoreDone = 'yes' #une fois OK\n",
    "    \n",
    "    browser.close()\n",
    "    \n",
    "    return(df)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
